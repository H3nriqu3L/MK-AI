{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c21ad23d",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Setup Jogo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b28d8c55",
   "metadata": {},
   "source": [
    "## Instalar Gym-Retro e Bibliotecas Necessarias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3de3bd82",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#!pip install gym==0.19.0 gym-retro "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc4a0b54",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu117"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e99f98d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# No prompt da anaconda ative o venv que ira utilizar, exemplo: \"conda activate envtest\". Apos ativar entre na sua pasta roms (com a ROM do jogo dentro dessa pasta) e execute o comando abaixo.\n",
    "# python -m retro.import ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be75b14d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#!pip install opencv-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "227e12a4-aeba-4b1a-a799-53a585851b5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install ipywidgets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f241cf40-4272-4224-a445-1a6a7719eef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install stable-baselines3[extra]==1.3.0 optuna --user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c17a7ca2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.7.16 (default, Jan 17 2023, 16:06:28) [MSC v.1916 64 bit (AMD64)]'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#python < 3.8 \n",
    "#Verifique que o python esta entre a versao 3.7 e 3.8 pois o gym retro so funciona nessas versoes\n",
    "import sys\n",
    "sys.version\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "974074e0",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Importar Dependencias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "07d8253b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#importando retro para usar MK ROM\n",
    "import retro\n",
    "# Sera usado para diminuir velocidade do jogo\n",
    "import time\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f25fdce7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Se erro do tipo ja existe um env aberto rodar esse codigo\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ec006ec7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Inicia jogo\n",
    "env = retro.make(game='MortalKombatII-Genesis')\n",
    "#env.reset()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b690c2ec",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Box([[[0 0 0]\n",
       "  [0 0 0]\n",
       "  [0 0 0]\n",
       "  ...\n",
       "  [0 0 0]\n",
       "  [0 0 0]\n",
       "  [0 0 0]]\n",
       "\n",
       " [[0 0 0]\n",
       "  [0 0 0]\n",
       "  [0 0 0]\n",
       "  ...\n",
       "  [0 0 0]\n",
       "  [0 0 0]\n",
       "  [0 0 0]]\n",
       "\n",
       " [[0 0 0]\n",
       "  [0 0 0]\n",
       "  [0 0 0]\n",
       "  ...\n",
       "  [0 0 0]\n",
       "  [0 0 0]\n",
       "  [0 0 0]]\n",
       "\n",
       " ...\n",
       "\n",
       " [[0 0 0]\n",
       "  [0 0 0]\n",
       "  [0 0 0]\n",
       "  ...\n",
       "  [0 0 0]\n",
       "  [0 0 0]\n",
       "  [0 0 0]]\n",
       "\n",
       " [[0 0 0]\n",
       "  [0 0 0]\n",
       "  [0 0 0]\n",
       "  ...\n",
       "  [0 0 0]\n",
       "  [0 0 0]\n",
       "  [0 0 0]]\n",
       "\n",
       " [[0 0 0]\n",
       "  [0 0 0]\n",
       "  [0 0 0]\n",
       "  ...\n",
       "  [0 0 0]\n",
       "  [0 0 0]\n",
       "  [0 0 0]]], [[[255 255 255]\n",
       "  [255 255 255]\n",
       "  [255 255 255]\n",
       "  ...\n",
       "  [255 255 255]\n",
       "  [255 255 255]\n",
       "  [255 255 255]]\n",
       "\n",
       " [[255 255 255]\n",
       "  [255 255 255]\n",
       "  [255 255 255]\n",
       "  ...\n",
       "  [255 255 255]\n",
       "  [255 255 255]\n",
       "  [255 255 255]]\n",
       "\n",
       " [[255 255 255]\n",
       "  [255 255 255]\n",
       "  [255 255 255]\n",
       "  ...\n",
       "  [255 255 255]\n",
       "  [255 255 255]\n",
       "  [255 255 255]]\n",
       "\n",
       " ...\n",
       "\n",
       " [[255 255 255]\n",
       "  [255 255 255]\n",
       "  [255 255 255]\n",
       "  ...\n",
       "  [255 255 255]\n",
       "  [255 255 255]\n",
       "  [255 255 255]]\n",
       "\n",
       " [[255 255 255]\n",
       "  [255 255 255]\n",
       "  [255 255 255]\n",
       "  ...\n",
       "  [255 255 255]\n",
       "  [255 255 255]\n",
       "  [255 255 255]]\n",
       "\n",
       " [[255 255 255]\n",
       "  [255 255 255]\n",
       "  [255 255 255]\n",
       "  ...\n",
       "  [255 255 255]\n",
       "  [255 255 255]\n",
       "  [255 255 255]]], (224, 320, 3), uint8)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Verificando se o observation space esta funcionando\n",
    "env.observation_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "25f0890d-ab4d-48e0-8be8-a9bae2ff622a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1], dtype=int8)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Verificando se o action space esta funcionando\n",
    "env.action_space.sample()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d524e55d",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Setup env\n",
    "Vamos otimizar removendo as cores (agente nao precisa ver cores) e diminuir observation space do agente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3919b2a5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from gym import Env \n",
    "from gym.spaces import MultiBinary, Box\n",
    "import numpy as np\n",
    "import cv2\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e0649ec7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Custom env, baseado em: https://stable-baselines.readthedocs.io/en/master/guide/custom_env.html\n",
    "class MortalKombat(Env):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.observation_space = Env.observation_space\n",
    "        self.observation_space = Box(low=0, high=255, shape=(84, 84, 1), dtype = np.uint8) #O que o agente \"ve\"\n",
    "        self.action_space = MultiBinary(12) # O que o agente pode fazer (acoes)\n",
    "        #self.game = retro.make(game='MortalKombatII-Genesis', obs_type='image', use_restricted_actions=retro.Actions.FILTERED) # retro.Actions.FILTERED nos da somente acoes validas\n",
    "        self.game = retro.make(game='MortalKombatII-Genesis', use_restricted_actions=retro.Actions.FILTERED ) # retro.Actions.FILTERED nos da somente acoes validas\n",
    "        #env = retro.make(gamename, state='firstfightveryeasy.state', obs_type=retro.Observations.IMAGE)\n",
    "        #self.game.reset()\n",
    "    def reset(self):\n",
    "        #Recomeca jogo\n",
    "        obs = self.game.reset()\n",
    "        obs = self.preprocess(obs)\n",
    "        self.previous_frame=obs\n",
    "        \n",
    "        # Infos (guardar o score a cada frame)\n",
    "        self.score = 0\n",
    "        return obs\n",
    "    \n",
    "    def render(self, *args, **kwargs):\n",
    "        self.game.render()\n",
    "    \n",
    "    def close(self):\n",
    "        self.game.close()\n",
    "        \n",
    "    \n",
    "    #Vamos transformar o jogo em cinza e diminuir o tamanho \n",
    "    def preprocess(self, observation):\n",
    "        #Transforma em cinza\n",
    "        cinza = cv2.cvtColor(observation, cv2.COLOR_BGR2GRAY)\n",
    "        #Resize\n",
    "        resize = cv2.resize(cinza, (84,84), interpolation = cv2.INTER_CUBIC)\n",
    "        final = np.reshape(resize, (84,84,1))\n",
    "        return final\n",
    "        \n",
    "    def step(self, action):\n",
    "        obs, self.reward, done, info = self.game.step(action)\n",
    "        obs = self.preprocess(obs) #Otimizacoes comentadas acimas...\n",
    "        \n",
    "        frame_change = obs - self.previous_frame #Pixels que mudaram do ultimo frame para o novo\n",
    "        self.previous_frame = obs\n",
    "        \n",
    "        #Reward (depois mudar os parametros)\n",
    "        self.reward = info['rounds_won']*500 + info['enemy_rounds_won']*-500 + info['health']*1 + info['enemy_health']*-1 - self.score\n",
    "        \n",
    "        \n",
    "        self.score = info['rounds_won']*500 + info['enemy_rounds_won']*-500 + info['health'] + info['enemy_health']*-1\n",
    "    \n",
    "        return frame_change, self.reward, done, info\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "db7d0b99",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "env.close()\n",
    "env = MortalKombat()\n",
    "#env.reset()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67b6ad2c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# IA faz movimento aleatorios nesse teste\n",
    "obs = env.reset()\n",
    "dead = False\n",
    "for game in range(1): \n",
    "    while not dead: \n",
    "        if dead: \n",
    "            obs = env.reset()\n",
    "        env.render()\n",
    "        obs, reward, dead, info = env.step(env.action_space.sample())\n",
    "        time.sleep(0.01) \n",
    "        print(reward)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c3e60e9",
   "metadata": {},
   "source": [
    "# Hiperparametros & Optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4ead7744",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA está disponível!\n",
      "Versão do CUDA: 11.7\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(\"CUDA está disponível!\")\n",
    "    print(\"Versão do CUDA:\", torch.version.cuda)\n",
    "else:\n",
    "    print(\"CUDA não está disponível.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9db29f59-b659-4f59-af50-375ce80aafc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.3.0\n"
     ]
    }
   ],
   "source": [
    "import stable_baselines3\n",
    "\n",
    "print(stable_baselines3.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "26d54b6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metodos e bibliotecas que utilizaremos\n",
    "import optuna\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv, VecFrameStack\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from stable_baselines3.common.vec_env import VecTransposeImage\n",
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "from stable_baselines3.common.vec_env import VecVideoRecorder\n",
    "from gym.wrappers import FrameStack\n",
    "from gym.wrappers import FrameStack\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d8ae4e0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "LOG_D = './logs/'\n",
    "OPT_D= './opt/'\n",
    "# Vamos salvar o resultados do Optuna nesse path\n",
    "PATH = os.path.join(OPT_D, 'v_{}_best_model'.format(1)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "529c0c31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./opt/v_1_best_model'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "03f5b38a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Usado pelo optuna para sugerir um valor para os hiperparametros\n",
    "#Valores sugeridos \n",
    "def opt_objective(trial): \n",
    "    return {\n",
    "        'n_steps':trial.suggest_int('n_steps', 2048, 8192),\n",
    "        'gamma':trial.suggest_float('gamma', 0.8, 0.9999),\n",
    "        'learning_rate':trial.suggest_float('learning_rate', 1e-5, 1e-4),\n",
    "        'clip_range':trial.suggest_float('clip_range', 0.1, 0.4),\n",
    "        'gae_lambda':trial.suggest_float('gae_lambda', 0.8, 0.99)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "789179ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_agent(trial):\n",
    "    try:\n",
    "        parametros = opt_objective(trial) # Pegamos valores para os hiperparametros da funcao testamos\n",
    "\n",
    "        env = MortalKombat()\n",
    "        #Monitora o env para depois podermos visualizar como as configuracoes de hiperparametros se sairam\n",
    "        env = Monitor(env, LOG_D)\n",
    "        env = DummyVecEnv([lambda: env])\n",
    "        #env = CustomDummyVecEnv([lambda: env])\n",
    "        # Nosso MDPs tera 4 frames de \"memoria\" para tomar a proxima decisao\n",
    "        env = VecFrameStack(env, n_stack=4, channels_order='last')\n",
    "        #env = FrameStack(env, 4)\n",
    "        #env = VecTransposeImage(env)\n",
    "\n",
    "        ppo_mod = PPO('CnnPolicy', env, tensorboard_log=LOG_D, verbose=0, **parametros) #utilizamos o PPO para resolver o MDPs\n",
    "        ppo_mod.learn(total_timesteps=100000)\n",
    "        #model.learn(total_timesteps=100)\n",
    "\n",
    "        mean_reward, _ = evaluate_policy(ppo_mod, env, n_eval_episodes=5) #n_eval_episodes é em quantos jogos essa configuracao sera testada\n",
    "        env.close()\n",
    "\n",
    "        PATH = os.path.join(OPT_D, 'v_{}_best_model'.format(trial.number))\n",
    "        ppo_mod.save(PATH)\n",
    "\n",
    "        return mean_reward \n",
    "\n",
    "    except Exception as e: #Algumas configuracoes de hiperparametros podem nao funcionar por isso a try\n",
    "        return -100000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5b78cee5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "env.close() #Para fechar um env caso esteja aberto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca6fedb0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Realizando o estudo de hiperparametros\n",
    "study = optuna.create_study(direction='maximize') #Por default o optuna tenta minimizar a funcao\n",
    "#study.optimize(optimize_agent, n_trials=100, n_jobs=1) \n",
    "study.optimize(optimize_agent, n_trials=5, n_jobs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afc99c16-9a2c-4791-9fff-2e69fbb60b87",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "number_x = study.best_trial.number\n",
    "print(number_x)\n",
    "path_v_best_model = 'v_{}_best_model.zip'.format(number_x)\n",
    "#path_v_best_model = 'v_{}_best_model.zip'.format('-1')\n",
    "best_model = PPO.load(os.path.join(OPT_D, path_v_best_model))\n",
    "print(path_v_best_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cefc7f2c",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "source": [
    "# Treinamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "95b0d711",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classe padrao baseada no callback da documentacao do stable baselines: https://stable-baselines3.readthedocs.io/en/master/guide/examples.html#basic-usage-training-saving-loading\n",
    "class CustomCallback(BaseCallback):\n",
    "\n",
    "    def __init__(self, check_freq, save_path, verbose=1):\n",
    "        super(CustomCallback, self).__init__(verbose)\n",
    "        self.check_freq = check_freq\n",
    "        self.save_path = save_path\n",
    "\n",
    "    def _init_callback(self):\n",
    "        if self.save_path is not None:\n",
    "            os.makedirs(self.save_path, exist_ok=True)\n",
    "\n",
    "    def _on_step(self):\n",
    "        if self.n_calls % self.check_freq == 0:\n",
    "            best_model_path = os.path.join(self.save_path, 'best_model_{}'.format(self.n_calls))\n",
    "            self.model.save(best_model_path)\n",
    "\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0ce20dae",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_D = './train/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d975eb52",
   "metadata": {},
   "outputs": [],
   "source": [
    "callback = CustomCallback(check_freq=10000, save_path=TRAIN_D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "897306f7-9c40-4b9f-b519-1f1e249d18bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "d0b21af1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "env = MortalKombat()\n",
    "env = Monitor(env, LOG_D)\n",
    "env = DummyVecEnv([lambda: env])\n",
    "env = VecFrameStack(env, n_stack=4)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1568abee-e966-400a-b2bf-175805731831",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "best_model_param = study.best_params\n",
    "best_model_param['n_steps'] = ((best_model_param['n_steps'] + 63) // 64) * 64 #tornando o n_steps divisivel por 64\n",
    "#Pode ser necessario diminuir a lrate caso nao esteja convergindo adequadamente\n",
    "best_model_param['learning_rate']= 1.182030842374264e-06\n",
    "best_model_param\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8864cf2b-d011-43b2-b129-d2249e892430",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n_steps': 7360,\n",
       " 'gamma': 0.8403833640495785,\n",
       " 'learning_rate': 4.04419273011491e-05,\n",
       " 'clip_range': 0.27718291020022345,\n",
       " 'gae_lambda': 0.8009321244173684}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Caso queira definir valores manualmente\n",
    "best_model_param = {'n_steps': 7360,\n",
    " 'gamma': 0.8403833640495785,\n",
    " 'learning_rate': 4.04419273011491e-05,\n",
    " 'clip_range': 0.27718291020022345,\n",
    " 'gae_lambda': 0.8009321244173684}    \n",
    "best_model_param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f783e842-e083-45da-92e1-7b9b85ce0db7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "Wrapping the env in a VecTransposeImage.\n"
     ]
    }
   ],
   "source": [
    "ppo_model = PPO('CnnPolicy', env, tensorboard_log=LOG_D, verbose=1, **best_model_param,ent_coef=0.01) #utilizamos o PPO para resolver o MDPs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "5203b64f-0050-4a9a-a89f-026701e0e3a9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.ppo.ppo.PPO at 0x176459f1548>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#ppo_model.load(os.path.join(OPT_D, path_v_best_model))\n",
    "ppo_model.load(os.path.join('./backup/backup1.zip'))\n",
    "#ppo_model.load(os.path.join(TRAIN_D, 'best_model_1000000.zip'))\n",
    "#print(path_v_best_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "5c1ec8cf-633e-4a14-8455-7cbd082a961c",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging to ./logs/PPO_7\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 2.94e+03 |\n",
      "|    ep_rew_mean     | -835     |\n",
      "| time/              |          |\n",
      "|    fps             | 386      |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 19       |\n",
      "|    total_timesteps | 7360     |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 2.44e+03   |\n",
      "|    ep_rew_mean          | -982       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 295        |\n",
      "|    iterations           | 2          |\n",
      "|    time_elapsed         | 49         |\n",
      "|    total_timesteps      | 14720      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01953918 |\n",
      "|    clip_fraction        | 0.119      |\n",
      "|    clip_range           | 0.277      |\n",
      "|    entropy_loss         | -8.07      |\n",
      "|    explained_variance   | 0.0171     |\n",
      "|    learning_rate        | 4.04e-05   |\n",
      "|    loss                 | 1.9e+03    |\n",
      "|    n_updates            | 150        |\n",
      "|    policy_gradient_loss | -0.0108    |\n",
      "|    value_loss           | 380        |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 2.54e+03    |\n",
      "|    ep_rew_mean          | -943        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 273         |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 80          |\n",
      "|    total_timesteps      | 22080       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018688897 |\n",
      "|    clip_fraction        | 0.11        |\n",
      "|    clip_range           | 0.277       |\n",
      "|    entropy_loss         | -8.04       |\n",
      "|    explained_variance   | 0.0184      |\n",
      "|    learning_rate        | 4.04e-05    |\n",
      "|    loss                 | 1.48        |\n",
      "|    n_updates            | 160         |\n",
      "|    policy_gradient_loss | -0.0119     |\n",
      "|    value_loss           | 429         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 2.57e+03    |\n",
      "|    ep_rew_mean          | -950        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 257         |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 114         |\n",
      "|    total_timesteps      | 29440       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017680485 |\n",
      "|    clip_fraction        | 0.0983      |\n",
      "|    clip_range           | 0.277       |\n",
      "|    entropy_loss         | -8.05       |\n",
      "|    explained_variance   | 0.0309      |\n",
      "|    learning_rate        | 4.04e-05    |\n",
      "|    loss                 | 2.44        |\n",
      "|    n_updates            | 170         |\n",
      "|    policy_gradient_loss | -0.0146     |\n",
      "|    value_loss           | 369         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 2.57e+03    |\n",
      "|    ep_rew_mean          | -950        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 252         |\n",
      "|    iterations           | 5           |\n",
      "|    time_elapsed         | 145         |\n",
      "|    total_timesteps      | 36800       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012564106 |\n",
      "|    clip_fraction        | 0.073       |\n",
      "|    clip_range           | 0.277       |\n",
      "|    entropy_loss         | -8.17       |\n",
      "|    explained_variance   | 0.00527     |\n",
      "|    learning_rate        | 4.04e-05    |\n",
      "|    loss                 | 0.415       |\n",
      "|    n_updates            | 180         |\n",
      "|    policy_gradient_loss | -0.0173     |\n",
      "|    value_loss           | 182         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 2.57e+03    |\n",
      "|    ep_rew_mean          | -950        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 248         |\n",
      "|    iterations           | 6           |\n",
      "|    time_elapsed         | 177         |\n",
      "|    total_timesteps      | 44160       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014282104 |\n",
      "|    clip_fraction        | 0.0362      |\n",
      "|    clip_range           | 0.277       |\n",
      "|    entropy_loss         | -8.3        |\n",
      "|    explained_variance   | -1.19e-07   |\n",
      "|    learning_rate        | 4.04e-05    |\n",
      "|    loss                 | -0.0829     |\n",
      "|    n_updates            | 190         |\n",
      "|    policy_gradient_loss | 0.0277      |\n",
      "|    value_loss           | 0.000183    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 2.57e+03    |\n",
      "|    ep_rew_mean          | -950        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 246         |\n",
      "|    iterations           | 7           |\n",
      "|    time_elapsed         | 209         |\n",
      "|    total_timesteps      | 51520       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016702052 |\n",
      "|    clip_fraction        | 0.0551      |\n",
      "|    clip_range           | 0.277       |\n",
      "|    entropy_loss         | -8.28       |\n",
      "|    explained_variance   | 2.38e-07    |\n",
      "|    learning_rate        | 4.04e-05    |\n",
      "|    loss                 | -0.0827     |\n",
      "|    n_updates            | 200         |\n",
      "|    policy_gradient_loss | 0.0192      |\n",
      "|    value_loss           | 5.51e-05    |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 2.57e+03   |\n",
      "|    ep_rew_mean          | -950       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 245        |\n",
      "|    iterations           | 8          |\n",
      "|    time_elapsed         | 240        |\n",
      "|    total_timesteps      | 58880      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01246457 |\n",
      "|    clip_fraction        | 0.058      |\n",
      "|    clip_range           | 0.277      |\n",
      "|    entropy_loss         | -8.28      |\n",
      "|    explained_variance   | -1.19e-07  |\n",
      "|    learning_rate        | 4.04e-05   |\n",
      "|    loss                 | -0.0828    |\n",
      "|    n_updates            | 210        |\n",
      "|    policy_gradient_loss | 0.0173     |\n",
      "|    value_loss           | 2.82e-05   |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 2.57e+03    |\n",
      "|    ep_rew_mean          | -950        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 243         |\n",
      "|    iterations           | 9           |\n",
      "|    time_elapsed         | 272         |\n",
      "|    total_timesteps      | 66240       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.024526265 |\n",
      "|    clip_fraction        | 0.104       |\n",
      "|    clip_range           | 0.277       |\n",
      "|    entropy_loss         | -8.26       |\n",
      "|    explained_variance   | -1.19e-07   |\n",
      "|    learning_rate        | 4.04e-05    |\n",
      "|    loss                 | -0.0825     |\n",
      "|    n_updates            | 220         |\n",
      "|    policy_gradient_loss | -0.00356    |\n",
      "|    value_loss           | 1.07e-05    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 2.57e+03    |\n",
      "|    ep_rew_mean          | -950        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 242         |\n",
      "|    iterations           | 10          |\n",
      "|    time_elapsed         | 303         |\n",
      "|    total_timesteps      | 73600       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015627097 |\n",
      "|    clip_fraction        | 0.0802      |\n",
      "|    clip_range           | 0.277       |\n",
      "|    entropy_loss         | -8.28       |\n",
      "|    explained_variance   | 5.96e-08    |\n",
      "|    learning_rate        | 4.04e-05    |\n",
      "|    loss                 | -0.0826     |\n",
      "|    n_updates            | 230         |\n",
      "|    policy_gradient_loss | -0.00253    |\n",
      "|    value_loss           | 1.07e-05    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 2.57e+03    |\n",
      "|    ep_rew_mean          | -950        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 241         |\n",
      "|    iterations           | 11          |\n",
      "|    time_elapsed         | 335         |\n",
      "|    total_timesteps      | 80960       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008414966 |\n",
      "|    clip_fraction        | 0.00935     |\n",
      "|    clip_range           | 0.277       |\n",
      "|    entropy_loss         | -8.25       |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 4.04e-05    |\n",
      "|    loss                 | -0.0825     |\n",
      "|    n_updates            | 240         |\n",
      "|    policy_gradient_loss | -0.0012     |\n",
      "|    value_loss           | 6e-06       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 2.57e+03    |\n",
      "|    ep_rew_mean          | -950        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 240         |\n",
      "|    iterations           | 12          |\n",
      "|    time_elapsed         | 366         |\n",
      "|    total_timesteps      | 88320       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017852778 |\n",
      "|    clip_fraction        | 0.0451      |\n",
      "|    clip_range           | 0.277       |\n",
      "|    entropy_loss         | -8.27       |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 4.04e-05    |\n",
      "|    loss                 | -0.119      |\n",
      "|    n_updates            | 250         |\n",
      "|    policy_gradient_loss | 0.000612    |\n",
      "|    value_loss           | 5.18e-06    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 2.57e+03    |\n",
      "|    ep_rew_mean          | -950        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 240         |\n",
      "|    iterations           | 13          |\n",
      "|    time_elapsed         | 397         |\n",
      "|    total_timesteps      | 95680       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015202145 |\n",
      "|    clip_fraction        | 0.0553      |\n",
      "|    clip_range           | 0.277       |\n",
      "|    entropy_loss         | -8.28       |\n",
      "|    explained_variance   | -1.19e-07   |\n",
      "|    learning_rate        | 4.04e-05    |\n",
      "|    loss                 | -0.0826     |\n",
      "|    n_updates            | 260         |\n",
      "|    policy_gradient_loss | -0.00099    |\n",
      "|    value_loss           | 8.55e-06    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 2.57e+03    |\n",
      "|    ep_rew_mean          | -950        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 239         |\n",
      "|    iterations           | 14          |\n",
      "|    time_elapsed         | 429         |\n",
      "|    total_timesteps      | 103040      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015808443 |\n",
      "|    clip_fraction        | 0.0644      |\n",
      "|    clip_range           | 0.277       |\n",
      "|    entropy_loss         | -8.29       |\n",
      "|    explained_variance   | 5.96e-08    |\n",
      "|    learning_rate        | 4.04e-05    |\n",
      "|    loss                 | -0.083      |\n",
      "|    n_updates            | 270         |\n",
      "|    policy_gradient_loss | -0.000732   |\n",
      "|    value_loss           | 4.06e-06    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 2.57e+03    |\n",
      "|    ep_rew_mean          | -950        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 239         |\n",
      "|    iterations           | 15          |\n",
      "|    time_elapsed         | 460         |\n",
      "|    total_timesteps      | 110400      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017601812 |\n",
      "|    clip_fraction        | 0.0947      |\n",
      "|    clip_range           | 0.277       |\n",
      "|    entropy_loss         | -8.27       |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 4.04e-05    |\n",
      "|    loss                 | -0.0827     |\n",
      "|    n_updates            | 280         |\n",
      "|    policy_gradient_loss | -0.00161    |\n",
      "|    value_loss           | 4.83e-06    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 2.57e+03    |\n",
      "|    ep_rew_mean          | -950        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 239         |\n",
      "|    iterations           | 16          |\n",
      "|    time_elapsed         | 491         |\n",
      "|    total_timesteps      | 117760      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012268422 |\n",
      "|    clip_fraction        | 0.0236      |\n",
      "|    clip_range           | 0.277       |\n",
      "|    entropy_loss         | -8.27       |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 4.04e-05    |\n",
      "|    loss                 | -0.0826     |\n",
      "|    n_updates            | 290         |\n",
      "|    policy_gradient_loss | -0.000783   |\n",
      "|    value_loss           | 5.2e-06     |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 2.57e+03   |\n",
      "|    ep_rew_mean          | -950       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 238        |\n",
      "|    iterations           | 17         |\n",
      "|    time_elapsed         | 523        |\n",
      "|    total_timesteps      | 125120     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03599082 |\n",
      "|    clip_fraction        | 0.199      |\n",
      "|    clip_range           | 0.277      |\n",
      "|    entropy_loss         | -8.3       |\n",
      "|    explained_variance   | 0          |\n",
      "|    learning_rate        | 4.04e-05   |\n",
      "|    loss                 | -0.0932    |\n",
      "|    n_updates            | 300        |\n",
      "|    policy_gradient_loss | -0.00152   |\n",
      "|    value_loss           | 3.43e-06   |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 2.57e+03    |\n",
      "|    ep_rew_mean          | -950        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 238         |\n",
      "|    iterations           | 18          |\n",
      "|    time_elapsed         | 556         |\n",
      "|    total_timesteps      | 132480      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017661382 |\n",
      "|    clip_fraction        | 0.0494      |\n",
      "|    clip_range           | 0.277       |\n",
      "|    entropy_loss         | -8.29       |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 4.04e-05    |\n",
      "|    loss                 | -0.0829     |\n",
      "|    n_updates            | 310         |\n",
      "|    policy_gradient_loss | -0.00103    |\n",
      "|    value_loss           | 2.12e-06    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 2.57e+03    |\n",
      "|    ep_rew_mean          | -950        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 237         |\n",
      "|    iterations           | 19          |\n",
      "|    time_elapsed         | 587         |\n",
      "|    total_timesteps      | 139840      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016698672 |\n",
      "|    clip_fraction        | 0.0443      |\n",
      "|    clip_range           | 0.277       |\n",
      "|    entropy_loss         | -8.27       |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 4.04e-05    |\n",
      "|    loss                 | -0.117      |\n",
      "|    n_updates            | 320         |\n",
      "|    policy_gradient_loss | -0.00129    |\n",
      "|    value_loss           | 2.1e-06     |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 2.57e+03    |\n",
      "|    ep_rew_mean          | -950        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 237         |\n",
      "|    iterations           | 20          |\n",
      "|    time_elapsed         | 620         |\n",
      "|    total_timesteps      | 147200      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.026300758 |\n",
      "|    clip_fraction        | 0.136       |\n",
      "|    clip_range           | 0.277       |\n",
      "|    entropy_loss         | -8.3        |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 4.04e-05    |\n",
      "|    loss                 | -0.104      |\n",
      "|    n_updates            | 330         |\n",
      "|    policy_gradient_loss | -0.000951   |\n",
      "|    value_loss           | 1.29e-06    |\n",
      "-----------------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.57e+03  |\n",
      "|    ep_rew_mean          | -950      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 236       |\n",
      "|    iterations           | 21        |\n",
      "|    time_elapsed         | 652       |\n",
      "|    total_timesteps      | 154560    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0148341 |\n",
      "|    clip_fraction        | 0.046     |\n",
      "|    clip_range           | 0.277     |\n",
      "|    entropy_loss         | -8.29     |\n",
      "|    explained_variance   | 5.96e-08  |\n",
      "|    learning_rate        | 4.04e-05  |\n",
      "|    loss                 | -0.0828   |\n",
      "|    n_updates            | 340       |\n",
      "|    policy_gradient_loss | -0.00139  |\n",
      "|    value_loss           | 2.63e-06  |\n",
      "---------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 2.57e+03    |\n",
      "|    ep_rew_mean          | -950        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 237         |\n",
      "|    iterations           | 22          |\n",
      "|    time_elapsed         | 683         |\n",
      "|    total_timesteps      | 161920      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022985734 |\n",
      "|    clip_fraction        | 0.135       |\n",
      "|    clip_range           | 0.277       |\n",
      "|    entropy_loss         | -8.31       |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 4.04e-05    |\n",
      "|    loss                 | -0.0831     |\n",
      "|    n_updates            | 350         |\n",
      "|    policy_gradient_loss | -0.000972   |\n",
      "|    value_loss           | 2.03e-06    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 2.57e+03    |\n",
      "|    ep_rew_mean          | -950        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 236         |\n",
      "|    iterations           | 23          |\n",
      "|    time_elapsed         | 714         |\n",
      "|    total_timesteps      | 169280      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012891254 |\n",
      "|    clip_fraction        | 0.0136      |\n",
      "|    clip_range           | 0.277       |\n",
      "|    entropy_loss         | -8.32       |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 4.04e-05    |\n",
      "|    loss                 | -0.0869     |\n",
      "|    n_updates            | 360         |\n",
      "|    policy_gradient_loss | -0.000637   |\n",
      "|    value_loss           | 1.25e-06    |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_5712\\3879423796.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mppo_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtotal_timesteps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1100000\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcallback\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mE:\\Anaconda\\envs\\python37\\lib\\site-packages\\stable_baselines3\\ppo\\ppo.py\u001b[0m in \u001b[0;36mlearn\u001b[1;34m(self, total_timesteps, callback, log_interval, eval_env, eval_freq, n_eval_episodes, tb_log_name, eval_log_path, reset_num_timesteps)\u001b[0m\n\u001b[0;32m    306\u001b[0m             \u001b[0mtb_log_name\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtb_log_name\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    307\u001b[0m             \u001b[0meval_log_path\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0meval_log_path\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 308\u001b[1;33m             \u001b[0mreset_num_timesteps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mreset_num_timesteps\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    309\u001b[0m         )\n",
      "\u001b[1;32mE:\\Anaconda\\envs\\python37\\lib\\site-packages\\stable_baselines3\\common\\on_policy_algorithm.py\u001b[0m in \u001b[0;36mlearn\u001b[1;34m(self, total_timesteps, callback, log_interval, eval_env, eval_freq, n_eval_episodes, tb_log_name, eval_log_path, reset_num_timesteps)\u001b[0m\n\u001b[0;32m    235\u001b[0m         \u001b[1;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnum_timesteps\u001b[0m \u001b[1;33m<\u001b[0m \u001b[0mtotal_timesteps\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    236\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 237\u001b[1;33m             \u001b[0mcontinue_training\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcollect_rollouts\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrollout_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_rollout_steps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_steps\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    238\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    239\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mcontinue_training\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Anaconda\\envs\\python37\\lib\\site-packages\\stable_baselines3\\common\\on_policy_algorithm.py\u001b[0m in \u001b[0;36mcollect_rollouts\u001b[1;34m(self, env, callback, rollout_buffer, n_rollout_steps)\u001b[0m\n\u001b[0;32m    167\u001b[0m                 \u001b[1;31m# Convert to pytorch tensor or to TensorDict\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    168\u001b[0m                 \u001b[0mobs_tensor\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mobs_as_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_last_obs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 169\u001b[1;33m                 \u001b[0mactions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlog_probs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpolicy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobs_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    170\u001b[0m             \u001b[0mactions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mactions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    171\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Anaconda\\envs\\python37\\lib\\site-packages\\stable_baselines3\\common\\policies.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, obs, deterministic)\u001b[0m\n\u001b[0;32m    588\u001b[0m         \u001b[1;31m# Evaluate the values for the given observations\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    589\u001b[0m         \u001b[0mvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalue_net\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlatent_vf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 590\u001b[1;33m         \u001b[0mdistribution\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_action_dist_from_latent\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlatent_pi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    591\u001b[0m         \u001b[0mactions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdistribution\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_actions\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdeterministic\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdeterministic\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    592\u001b[0m         \u001b[0mlog_prob\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdistribution\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog_prob\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mactions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Anaconda\\envs\\python37\\lib\\site-packages\\stable_baselines3\\common\\policies.py\u001b[0m in \u001b[0;36m_get_action_dist_from_latent\u001b[1;34m(self, latent_pi)\u001b[0m\n\u001b[0;32m    612\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maction_dist\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mBernoulliDistribution\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    613\u001b[0m             \u001b[1;31m# Here mean_actions are the logits (before rounding to get the binary actions)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 614\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maction_dist\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mproba_distribution\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction_logits\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmean_actions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    615\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maction_dist\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mStateDependentNoiseDistribution\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    616\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maction_dist\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mproba_distribution\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmean_actions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog_std\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlatent_pi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Anaconda\\envs\\python37\\lib\\site-packages\\stable_baselines3\\common\\distributions.py\u001b[0m in \u001b[0;36mproba_distribution\u001b[1;34m(self, action_logits)\u001b[0m\n\u001b[0;32m    377\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    378\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mproba_distribution\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maction_logits\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mth\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[1;34m\"BernoulliDistribution\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 379\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdistribution\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mBernoulli\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maction_logits\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    380\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    381\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Anaconda\\envs\\python37\\lib\\site-packages\\torch\\distributions\\bernoulli.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, probs, logits, validate_args)\u001b[0m\n\u001b[0;32m     49\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     50\u001b[0m             \u001b[0mbatch_shape\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_param\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 51\u001b[1;33m         \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mBernoulli\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_shape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidate_args\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidate_args\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     52\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mexpand\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_shape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_instance\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Anaconda\\envs\\python37\\lib\\site-packages\\torch\\distributions\\distribution.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, batch_shape, event_shape, validate_args)\u001b[0m\n\u001b[0;32m     53\u001b[0m                 \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparam\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     54\u001b[0m                 \u001b[0mvalid\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconstraint\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcheck\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 55\u001b[1;33m                 \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mvalid\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     56\u001b[0m                     raise ValueError(\n\u001b[0;32m     57\u001b[0m                         \u001b[1;34mf\"Expected parameter {param} \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "ppo_model.learn(total_timesteps=1100000, callback=callback)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7257e23d-23e2-4921-b274-165224a75af2",
   "metadata": {},
   "source": [
    "# Avaliar Modelos Treinados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "07f61f68-9580-47af-acfe-44309a51dcee",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrapping the env in a VecTransposeImage.\n"
     ]
    }
   ],
   "source": [
    "#path_train = './opt/v_-1_best_model.zip'\n",
    "#path_train = './train/best_model_260000.zip'\n",
    "path_train = './backup/backup1.zip'\n",
    "ppo_model = PPO.load(path_train, env=env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "5d33e73e-bb35-4677-b155-9919b5e0e0ee",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1120.0\n"
     ]
    }
   ],
   "source": [
    "mean_score, _ = evaluate_policy(ppo_model, env, n_eval_episodes=2, render=True, deterministic=True)\n",
    "print (mean_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "668a5528-f3da-4b14-9927-d75a69c24a16",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b6d4871-aba3-46f0-b5d8-d7eed269844e",
   "metadata": {},
   "source": [
    "# Testes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "914c0992-ea20-45fa-977a-b2d4f4cfc03e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20.]\n",
      "[20.]\n",
      "[80.]\n",
      "[20.]\n",
      "[24.]\n",
      "[20.]\n",
      "[20.]\n",
      "[20.]\n",
      "[16.]\n",
      "[500.]\n",
      "[20.]\n",
      "[20.]\n",
      "[15.]\n",
      "[20.]\n"
     ]
    }
   ],
   "source": [
    "obs = env.reset()\n",
    "dead = False\n",
    "for game in range(1): \n",
    "    while not dead: \n",
    "        if dead: \n",
    "            obs = env.reset()\n",
    "        env.render()\n",
    "        predict = ppo_model.predict(obs)[0]\n",
    "        obs, reward, dead, info = env.step(predict)\n",
    "        time.sleep(0.001)\n",
    "        total_reward+=reward\n",
    "        cont+=1\n",
    "        if reward>0:\n",
    "            print(reward)\n",
    "            \n",
    "#print(total_reward)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2382ab1-cc10-4a87-9c98-21ef1e79e392",
   "metadata": {},
   "outputs": [],
   "source": [
    "ppo_model.save('./backup/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "911da0e1-ed67-4ab1-9ccd-0d9579699fdf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
